{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5hKsT2pwGx07tgv0f4g37"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_NxjOgPknPO","executionInfo":{"status":"ok","timestamp":1762405856709,"user_tz":-540,"elapsed":7736,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"74290baa-64ae-4d22-8469-c0b82b6dbdbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["파이토치 버전: 2.8.0+cu126\n"]}],"source":["from importlib.metadata import version\n","\n","import torch\n","print(\"파이토치 버전:\", version(\"torch\"))"]},{"cell_type":"markdown","source":["# 연습문제 3.1"],"metadata":{"id":"q6NIJCIOkxFN"}},{"cell_type":"code","source":["inputs = torch.tensor([\n","    [0.43, 0.15, 0.89], # Your (x^1)\n","    [0.55, 0.87, 0.66], # journey (x^2)\n","    [0.57, 0.85, 0.64], # starts (x^3)\n","    [0.22, 0.58, 0.33], # with (x^4)\n","    [0.77, 0.25, 0.10], # one (x^5)\n","    [0.05, 0.80, 0.55], # step (x^6)\n","])\n","\n","d_in, d_out = 3, 2"],"metadata":{"id":"_Y-P_lRBkwbj","executionInfo":{"status":"ok","timestamp":1762407052541,"user_tz":-540,"elapsed":42,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class SelfAttention_v1(nn.Module):\n","\n","  def __init__(self, d_in, d_out):\n","    super().__init__()\n","    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n","    self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n","    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n","\n","  def forward(self, x):\n","    keys = x @ self.W_key\n","    queries = x @ self.W_query\n","    values = x @ self.W_value\n","\n","    attn_scores = queries @ keys.T # omega\n","    attn_weights = torch.softmax(\n","        attn_scores / keys.shape[-1]**0.5, dim=-1\n","    )\n","\n","    context_vec = attn_weights @ values\n","    return context_vec\n","\n","torch.manual_seed(123)\n","sa_v1 = SelfAttention_v1(d_in, d_out)"],"metadata":{"id":"sYK4QEn3lS9p","executionInfo":{"status":"ok","timestamp":1762407212264,"user_tz":-540,"elapsed":7,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class SelfAttention_v2(nn.Module):\n","\n","  def __init__(self, d_in, d_out, qkv_bias=False):\n","    super().__init__()\n","    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","\n","  def forward(self, x):\n","    keys = self.W_key(x)\n","    queries = self.W_query(x)\n","    values = self.W_value(x)\n","\n","    attn_scores = queries @ keys.T # omega\n","    attn_weights = torch.softmax(\n","        attn_scores / keys.shape[-1]**0.5, dim=-1\n","    )\n","\n","    context_vec = attn_weights @ values\n","    return context_vec\n","\n","torch.manual_seed(123)\n","sa_v2 = SelfAttention_v2(d_in, d_out)"],"metadata":{"id":"vK0eFPO7phPa","executionInfo":{"status":"ok","timestamp":1762407213562,"user_tz":-540,"elapsed":5,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["sa_v1.W_query = torch.nn.Parameter(sa_v2.W_query.weight.T)\n","sa_v1.W_key = torch.nn.Parameter(sa_v2.W_key.weight.T)\n","sa_v1.W_value = torch.nn.Parameter(sa_v2.W_value.weight.T)"],"metadata":{"id":"v8RZE6c2pmY1","executionInfo":{"status":"ok","timestamp":1762407214773,"user_tz":-540,"elapsed":7,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["sa_v1(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuOCR1ERp1_j","executionInfo":{"status":"ok","timestamp":1762407215156,"user_tz":-540,"elapsed":11,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"665d20b3-4de4-46b0-f6f2-a1b6f3a4fcb1"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.5337, -0.1051],\n","        [-0.5323, -0.1080],\n","        [-0.5323, -0.1079],\n","        [-0.5297, -0.1076],\n","        [-0.5311, -0.1066],\n","        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["sa_v2(inputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MC96k33p2Gk","executionInfo":{"status":"ok","timestamp":1762407216866,"user_tz":-540,"elapsed":8,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"2ac70973-d3f7-42a2-daad-41aa640671f9"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.5337, -0.1051],\n","        [-0.5323, -0.1080],\n","        [-0.5323, -0.1079],\n","        [-0.5297, -0.1076],\n","        [-0.5311, -0.1066],\n","        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# 연습 문제 3.2"],"metadata":{"id":"yTD5Sk9fpRMV"}},{"cell_type":"code","source":["class CausalAttention(nn.Module):\n","  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n","    super().__init__()\n","    self.d_out = d_out\n","    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.dropout = nn.Dropout(dropout) # 추가\n","    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # 추가\n","\n","  def forward(self, x):\n","    b, num_tokens, d_in = x.shape # b: 배치 차원\n","    # 입력의 `num_tokens`가 `context_length`를 넘는 경우 마스크 생성에서 오류가 발생\n","    # 실제로는 forward 메서드에 들어괴 전에 LLM이 입력이 `context_length`를\n","    # 넘지 않는지 확인하기 때문에 문제가 되지 않는다.\n","    keys = self.W_key(x)\n","    queries = self.W_query(x)\n","    values = self.W_value(x)\n","\n","    attn_scores = queries @ keys.transpose(1, 2) # 전치\n","    attn_scores.masked_fill( # _ 메서드는 인플레이스 연산\n","        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n","    attn_weights = torch.softmax(\n","        attn_scores / keys.shape[-1]**0.5, dim=-1\n","    )\n","    attn_weights = self.dropout(attn_weights) # 추가\n","\n","    context_vec = attn_weights @ values\n","    return context_vec\n","\n","class MultiHeadAttentionWrapper(nn.Module):\n","  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","    super().__init__()\n","    self.heads = nn.ModuleList(\n","        [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n","    )\n","    self.out_proj = nn.Linear(d_out*num_heads, d_out*num_heads)\n","\n","  def forward(self, x):\n","    context_vec = torch.cat([head(x) for head in self.heads], dim=-1)\n","    return self.out_proj(context_vec)"],"metadata":{"id":"4picXkGTpVcd","executionInfo":{"status":"ok","timestamp":1762407606257,"user_tz":-540,"elapsed":3,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","\n","inputs = torch.tensor([\n","    [0.43, 0.15, 0.89], # Your (x^1)\n","    [0.55, 0.87, 0.66], # journey (x^2)\n","    [0.57, 0.85, 0.64], # starts (x^3)\n","    [0.22, 0.58, 0.33], # with (x^4)\n","    [0.77, 0.25, 0.10], # one (x^5)\n","    [0.05, 0.80, 0.55], # step (x^6)\n","])\n","\n","batch = torch.stack((inputs, inputs), dim=0)\n","\n","context_length = batch.shape[1]\n","d_in = 3\n","d_out = 1\n","\n","mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n","\n","context_vecs = mha(batch)\n","\n","print(context_vecs)\n","print(\"context_vecs.shape:\", context_vecs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sgQZCxvyqPcd","executionInfo":{"status":"ok","timestamp":1762407652586,"user_tz":-540,"elapsed":65,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"0a2b9421-63e9-48b3-903c-28684e9d7952"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.2770, 0.3472],\n","         [0.2767, 0.3471],\n","         [0.2767, 0.3471],\n","         [0.2764, 0.3474],\n","         [0.2767, 0.3474],\n","         [0.2763, 0.3473]],\n","\n","        [[0.2770, 0.3472],\n","         [0.2767, 0.3471],\n","         [0.2767, 0.3471],\n","         [0.2764, 0.3474],\n","         [0.2767, 0.3474],\n","         [0.2763, 0.3473]]], grad_fn=<ViewBackward0>)\n","context_vecs.shape: torch.Size([2, 6, 2])\n"]}]},{"cell_type":"markdown","source":["# 연습 문제 3.3"],"metadata":{"id":"rTyy05x9pRVt"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","    super().__init__()\n","    assert (d_out % num_heads == 0), \"d_out은 num_heads로 나누어 떨어져야 합니다\"\n","\n","    self.d_out = d_out\n","    self.num_heads = num_heads\n","    self.head_dim = d_out // num_heads # 원하는 출력 차원에 맞도록 투영 차원을 낮춘다.\n","\n","    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.out_proj = nn.Linear(d_out, d_out) # Linear 층을 사용해 헤드의 출력을 결합\n","    self.dropout = nn.Dropout(dropout)\n","    self.register_buffer(\n","        \"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1)\n","    )\n","\n","  def forward(self, x):\n","    b, num_tokens, d_in = x.shape\n","    # `CausalAttention`과 마찬가지로, 입력의 `num_tokens`가 `context_length`를 넘는 경우 마스크 생성에서 오류가 발생\n","    # 실제로는 forward 메서드에 들어오기 전에 LLM이 입력이 `context_length`를 넘지 않는지 확인하기 때문에 문제가 되지 않는다.\n","\n","    keys = self.W_key(x) # 크기: (b, num_tokens, d_out)\n","    queries = self.W_query(x)\n","    values = self.W_value(x)\n","\n","    # `num_heads` 차원을 추가함으로써 암묵적으로 행렬을 분할한다.\n","    # 그다음 마지막 차원을 `num_heads`에 맞춰 채운다.: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","    # 전치: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","    keys = keys.transpose(1,2)\n","    queries = queries.transpose(1,2)\n","    values = values.transpose(1,2)\n","\n","    # 코잘 마스크로 스케일드 점곱 어텐션(셀프 어텐션)을 계산\n","    attn_scores = queries @ keys.transpose(2,3) # 각 헤드에 대해 점곱을 수행\n","\n","    # 마스크를 불리언 타입으로 만들고 토큰 개수로 마스크를 자른다.\n","    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","    # 마스크를 사용해 어텐션 점수를 채운다.\n","    attn_scores.masked_fill(mask_bool, -torch.inf)\n","\n","    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","    attn_weights = self.dropout(attn_weights)\n","\n","    # 크기: (b, num_tokens, num_heads, head_dim)\n","    context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","    # 헤드를 결합한다. self.d_out = self.num_heads * self.head_dim\n","    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","    context_vec = self.out_proj(context_vec) # 투영\n","\n","    return context_vec"],"metadata":{"id":"7ozLSh8tpVl1","executionInfo":{"status":"ok","timestamp":1762407832098,"user_tz":-540,"elapsed":49,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["context_length = 1024\n","d_in, d_out = 768, 768\n","num_heads = 12\n","\n","mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads)"],"metadata":{"id":"dZ0XTsBurzYE","executionInfo":{"status":"ok","timestamp":1762407923299,"user_tz":-540,"elapsed":65,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["def count_parameteres(model):\n","  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","count_parameteres(mha)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zt9N3mpRr_5t","executionInfo":{"status":"ok","timestamp":1762407924637,"user_tz":-540,"elapsed":12,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"99807b2a-455e-47d5-d637-d835f31503df"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2360064"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":[],"metadata":{"id":"dJ4vOVF1sJ6t"},"execution_count":null,"outputs":[]}]}