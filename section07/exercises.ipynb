{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQsv/qtv81+3+It9kzjwLJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 연습문제 7.1: 프롬프트 스타일 변경"],"metadata":{"id":"-N5qd7vZ16SN"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"w980cmjvmf28","executionInfo":{"status":"ok","timestamp":1763971480665,"user_tz":-540,"elapsed":6,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"outputs":[],"source":["def format_input(entry):\n","  instruction_text = (\n","      f\"<|user|>\\n{entry['instruction']}\"\n","  )\n","\n","  input_text = f\"\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","\n","  return instruction_text + input_text"]},{"cell_type":"code","source":["sample_data = [\n","    {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"},\n","    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n","]\n","\n","print(format_input(sample_data[0]))\n","print()\n","print(format_input(sample_data[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e4WJREwi2bVe","executionInfo":{"status":"ok","timestamp":1763971480675,"user_tz":-540,"elapsed":3,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"cd1e1cf0-906b-46ad-d3ef-60694ae853cc"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<|user|>\n","Identify the correct spelling of the following word.\n","Ocassion\n","\n","<|user|>\n","What is an antonym of 'complicated'?\n"]}]},{"cell_type":"markdown","source":["# 연습문제 7.2: 명령어 및 입력 마스킹"],"metadata":{"id":"46PcuJeM495y"}},{"cell_type":"code","source":["# 이 `format_input` 함수는 원래 7장 코드에서 복사되었습니다.\n","def format_input(entry):\n","    instruction_text = (\n","        f\"Below is an instruction that describes a task. \"\n","        f\"Write a response that appropriately completes the request.\"\n","        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n","    )\n","\n","    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n","\n","    return instruction_text + input_text"],"metadata":{"id":"HrrTOXk42fDt","executionInfo":{"status":"ok","timestamp":1763971480682,"user_tz":-540,"elapsed":6,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","\n","class InstructionDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","\n","        ##########################################################################################\n","        # 추가: 지시 길이를 위한 별도의 리스트\n","        self.instruction_lengths = []\n","        ##########################################################################################\n","\n","        self.encoded_texts = []\n","\n","        for entry in data:\n","            instruction_plus_input = format_input(entry)\n","            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n","            full_text = instruction_plus_input + response_text\n","\n","            self.encoded_texts.append(\n","                tokenizer.encode(full_text)\n","            )\n","\n","            ##########################################################################################\n","            # 추가: 지시 길이 수집\n","            instruction_length = len(tokenizer.encode(instruction_plus_input))\n","            self.instruction_lengths.append(instruction_length)\n","            ##########################################################################################\n","\n","    def __getitem__(self, index):\n","        # 추가: 지시 길이와 텍스트를 모두 따로 반환\n","        return self.instruction_lengths[index], self.encoded_texts[index]\n","\n","    def __len__(self):\n","        return len(self.data)"],"metadata":{"id":"3DDi7W5X5EcY","executionInfo":{"status":"ok","timestamp":1763971485445,"user_tz":-540,"elapsed":4756,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import tiktoken\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")"],"metadata":{"id":"mIBNywxO5LZD","executionInfo":{"status":"ok","timestamp":1763971496292,"user_tz":-540,"elapsed":1051,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def custom_collate_fn(\n","    batch,\n","    pad_token_id=50256,\n","    ignore_index=-100,\n","    allowed_max_length=None,\n","    device=\"cpu\"\n","):\n","    # 배치에서 가장 긴 시퀀스 찾기\n","    batch_max_length = max(len(item)+1 for instruction_length, item in batch)   # 추가: batch는 이제 튜플입니다.\n","\n","    # 입력과 타깃을 패딩하고 준비\n","    inputs_lst, targets_lst = [], []\n","\n","    for instruction_length, item in batch:  # 추가: batch는 이제 튜플입니다.\n","        new_item = item.copy()\n","        # <|endoftext|> 토큰 추가\n","        new_item += [pad_token_id]\n","        # 시퀀스를 max_length까지 패딩\n","        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n","        inputs = torch.tensor(padded[:-1])  # 입력을 위해 마지막 토큰 자르기\n","        targets = torch.tensor(padded[1:])  # 타깃을 위해 오른쪽으로 +1 이동\n","\n","        # 타깃에서 첫 번째를 제외한 모든 패딩 토큰을 ignore_index로 바꾸기\n","        mask = targets == pad_token_id\n","        indices = torch.nonzero(mask).squeeze()\n","        if indices.numel() > 1:\n","            targets[indices[1:]] = ignore_index\n","\n","        ##########################################################################################\n","        # 추가: 타깃에서 모든 입력 및 지시 토큰 마스킹\n","        targets[:instruction_length-1] = -100\n","        ##########################################################################################\n","\n","        # 선택적으로 최대 시퀀스 길이로 자르기\n","        if allowed_max_length is not None:\n","            inputs = inputs[:allowed_max_length]\n","            targets = targets[:allowed_max_length]\n","\n","        inputs_lst.append(inputs)\n","        targets_lst.append(targets)\n","\n","    # 입력 및 타깃 리스트를 텐서로 변환하고 타깃 장치로 전송\n","    inputs_tensor = torch.stack(inputs_lst).to(device)\n","    targets_tensor = torch.stack(targets_lst).to(device)\n","\n","    return inputs_tensor, targets_tensor"],"metadata":{"id":"QUS4Yf545PSF","executionInfo":{"status":"ok","timestamp":1763971505404,"user_tz":-540,"elapsed":3,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["sample_data = [\n","    {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"},\n","    {'instruction': 'Sort the following list in alphabetical order.', 'input': 'Zebra, Elephant, Crocodile', 'output': 'Crocodile, Elephant, Zebra'},\n","    {'instruction': 'Arrange the given numbers in descending order.', 'input': '5, 12, 8, 3, 15', 'output': '15, 12, 8, 5, 3.'}\n","]\n"],"metadata":{"id":"-vMfvT3T5Rwt","executionInfo":{"status":"ok","timestamp":1763971523844,"user_tz":-540,"elapsed":40,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","train_dataset = InstructionDataset(sample_data, tokenizer)\n","train_loader = DataLoader(\n","    train_dataset,\n","    batch_size=len(sample_data),\n","    collate_fn=custom_collate_fn,\n","    num_workers=0\n",")"],"metadata":{"id":"-O-Qsbny5WQP","executionInfo":{"status":"ok","timestamp":1763971532711,"user_tz":-540,"elapsed":16,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(\"훈련 데이터 로더:\")\n","for inputs, targets in train_loader:\n","    print(inputs.shape, targets.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OSPijUA85YbD","executionInfo":{"status":"ok","timestamp":1763971540638,"user_tz":-540,"elapsed":114,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"25d04947-2df9-48fa-ac6e-1d37dfc4b17d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 데이터 로더:\n","torch.Size([3, 64]) torch.Size([3, 64])\n"]}]},{"cell_type":"code","source":["print(\"입력:\\n\", inputs[1])\n","print(\"\\n\\n타깃:\\n\", targets[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QWhhVqm-5aVl","executionInfo":{"status":"ok","timestamp":1763971545848,"user_tz":-540,"elapsed":9,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"90796dfa-5fc6-4fce-8471-22ebfeeca58e"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["입력:\n"," tensor([21106,   318,   281, 12064,   326,  8477,   257,  4876,    13, 19430,\n","          257,  2882,   326, 20431, 32543,   262,  2581,    13,   198,   198,\n","        21017, 46486,    25,   198, 42758,   262,  1708,  1351,   287, 24830,\n","          605,  1502,    13,   198,   198, 21017, 23412,    25,   198,    57,\n","        37052,    11, 42651,    11,  9325, 19815,   576,   198,   198, 21017,\n","        18261,    25,   198,    34, 12204,   375,   576,    11, 42651,    11,\n","         1168, 37052, 50256, 50256])\n","\n","\n","타깃:\n"," tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","         -100,  -100,  -100,  -100,  -100,  -100,   198,   198, 21017, 18261,\n","           25,   198,    34, 12204,   375,   576,    11, 42651,    11,  1168,\n","        37052, 50256,  -100,  -100])\n"]}]},{"cell_type":"code","source":["print(tokenizer.decode(list(inputs[1])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfnEV86r5bos","executionInfo":{"status":"ok","timestamp":1763971561674,"user_tz":-540,"elapsed":7,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"f2d6b08c-41c2-48e6-c571-5e9077c578b2"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Below is an instruction that describes a task. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","Sort the following list in alphabetical order.\n","\n","### Input:\n","Zebra, Elephant, Crocodile\n","\n","### Response:\n","Crocodile, Elephant, Zebra<|endoftext|><|endoftext|>\n"]}]},{"cell_type":"code","source":["non_masked_targets = targets[1][targets[1] != -100]\n","\n","print(tokenizer.decode(list(non_masked_targets)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGQ-W-Qc5fgA","executionInfo":{"status":"ok","timestamp":1763971573639,"user_tz":-540,"elapsed":42,"user":{"displayName":"장연식","userId":"03599627759361599230"}},"outputId":"c472e2a8-fd14-4487-c959-f0d2104eefc3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","### Response:\n","Crocodile, Elephant, Zebra<|endoftext|>\n"]}]},{"cell_type":"markdown","source":["# 연습문제 7.3: 원본 Alpaca 데이터셋에서 파인튜닝"],"metadata":{"id":"hfAdCcNe5n5F"}},{"cell_type":"code","source":["# 해당 데이터로 훈련\n","url = \"https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\""],"metadata":{"id":"VRy7M52T5iaX","executionInfo":{"status":"ok","timestamp":1763971691260,"user_tz":-540,"elapsed":6,"user":{"displayName":"장연식","userId":"03599627759361599230"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# 연습문제 7.4: LoRA를 사용한 파라미터 효율적인 미세 튜닝"],"metadata":{"id":"4YHtTzlT6Tsk"}}]}